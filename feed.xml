<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://mcphersonianoliver.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://mcphersonianoliver.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-26T12:49:44+00:00</updated><id>https://mcphersonianoliver.github.io/feed.xml</id><title type="html">Ian McPherson</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Proximal Operators - Optimization Perspective</title><link href="https://mcphersonianoliver.github.io/blog/2024/placeholder/" rel="alternate" type="text/html" title="Proximal Operators - Optimization Perspective"/><published>2024-12-06T16:40:16+00:00</published><updated>2024-12-06T16:40:16+00:00</updated><id>https://mcphersonianoliver.github.io/blog/2024/placeholder</id><content type="html" xml:base="https://mcphersonianoliver.github.io/blog/2024/placeholder/"><![CDATA[<p>In many applications and domains, one can boil down the crux of a problem to one of optimization. For simplicity, assume we are optimizing a function \(f: \mathbb{R}^d \rightarrow \mathbb{R}\) over \((\mathbb{R},\lVert \cdot \rVert_2)\) that has nice properties - for instance, convex, lower-semi continuous, etc. Without loss of generality, suppose we wish to minimize said function:</p> \[\min_{x \in \mathbb{R}^d} f(x).\] <p>For exposition’s sake, assume further that \(f \in C^1(\mathbb{R}^d)\), then one can do a <em>gradient descent</em> style algorithm. That is, starting from some initialization \(x_0\), one obtains the sequence of iterates \(\{x_n\}_{n \geq 0}\) in an iterative fashion:</p> \[x_{n+1} = x_{n} - \alpha_{n+1} \nabla f(x_n),\] <p>where \(\{\alpha_n\}_{n \geq 0}\) is a sequence of step-sizes.</p> <p>Because of the easy of statement, and the prevalence within many machine learning centric disciplines, gradient descent - and it’s closely related optimization relatives - enjoys a lot of publicity and study. Serving also as a way of teaching student’s how to prove different types of convergence rates, and how a function classes’ characteristics effect the outcome, gradient descent style algorithms rightfully take much of the spotlight!</p> <p>This being said, today I wanted to focus on the lesser known <em>proximal point method</em> and the corresponding <em>proximal operator</em>. In some communities, where gradient descent is seen as <em>forward euler</em>, the natural foil is <em>backward euler</em> - which is actually the <em>proximal point method</em>! If you have taken any <em>numerical analysis</em> or <em>numerical PDEs</em> course, it should come out no surprise that this method comes with many theoretical benefits. Beyond nice benefits regarding numerical stability, the proximal operator itself is of much interest for theoretical applications. The main theoretical application we wish to build towards is that of establishing the notion of a <em>gradient flow in 2-Wasserstein space</em>. Before leaving theory, we highlight how we may allow ourselves to change our geometric interpretation of <em>proximal</em>. By doing so, we can also make connections with general <em>regularization based ideas</em> that often play a role in signal processing and statistical learning adjacent fields. Lastly, while these methods are generally impractical for general functions, we will highlight ideas for making them more computationally feasible - with a slight payment of using a surrogate for our functions of interest.</p> <h2 id="proximal-operator-and-proximal-point-method">Proximal Operator and Proximal Point Method</h2>]]></content><author><name></name></author><category term="Optimization"/><category term="Foundations"/><category term="optimization"/><category term="gradient-flows"/><summary type="html"><![CDATA[placeholder]]></summary></entry></feed>